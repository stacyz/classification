{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cross_validation import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "For this exercise, we will be using a dataset of housing prices in Boston during the 1970s. Python's super-awesome sklearn package already has the data we need to get started. Below is the command to load the data. The data is stored as a dictionary. \n",
    "\n",
    "The 'DESCR' is a description of the data and the command for printing it is below. Note all the features we have to work with. From the dictionary, we need the data and the target variable (in this case, housing price). Store these as variables named \"data\" and \"price\", respectively. Once you have these, print their shapes to see all checks out with the DESCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "print (boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = boston.data\n",
    "price = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "(506,)\n"
     ]
    }
   ],
   "source": [
    "print (data.shape)\n",
    "print (price.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test split\n",
    "\n",
    "Now, using sklearn's train_test_split, (see [here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for more. I've already imported it for you.) let's make a random train-test split with the test size equal to 30% of our data (i.e. set the test_size parameter to 0.3). For consistency, let's also set the random.state parameter = 11.\n",
    "\n",
    "Name the variables train_data, train_price for the training data and test_data, test_price for the test data. As a sanity check, let's also print the shapes of these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_train, data_test, price_train, price_test = train_test_split(data, price, test_size=0.30, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 13)\n",
      "(152, 13)\n",
      "(354,)\n",
      "(152,)\n"
     ]
    }
   ],
   "source": [
    "print(data_train.shape)\n",
    "print(data_test.shape)\n",
    "print(price_train.shape)\n",
    "print(price_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale our data\n",
    "\n",
    "Before we get too far ahead, let's scale our data. Let's subtract the min from each column (feature) and divide by the difference between the max and min for each column. \n",
    "\n",
    "Here's where things can get tricky. Remember, our test data is unseen yet we need to scale it. We cannot scale using it's min/max because the data is unseen might not be available to us en masse. Instead, we use the training min/max to scale the test data.\n",
    "\n",
    "Be sure to check which axis you use to take the mins/maxes!\n",
    "\n",
    "Let's add a \"\\_stand\" suffix to our train/test variable names for the standardized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mins = np.min(data_train, axis = 0)\n",
    "maxes = np.max(data_train, axis = 0)\n",
    "diff = maxes - mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.89698800e+01,   9.50000000e+01,   2.72800000e+01,\n",
       "         1.00000000e+00,   4.86000000e-01,   4.86200000e+00,\n",
       "         9.38000000e+01,   1.09969000e+01,   2.30000000e+01,\n",
       "         5.24000000e+02,   9.40000000e+00,   3.96580000e+02,\n",
       "         3.62400000e+01])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train_stand = (data_train - mins) / diff\n",
    "data_tst_stand = (data_test - mins) / diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minPrice = np.min(price_train)\n",
    "maxPrice = np.max(price_train)\n",
    "diffPrice = maxPrice - minPrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "price_train_stand = (price_train - minPrice) / diffPrice\n",
    "price_test_stand = (price_train - minPrice) / diffPrice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold CV\n",
    "\n",
    "Now, here's where things might get really messy. Let's implement 10-Fold Cross Validation on K-NN across a range of K values (given below - 9 total). We'll keep our K for K-fold CV constant at 10. \n",
    "\n",
    "Let's determine our accuracy using an RMSE (root-mean-square-error) value based on Euclidean distance. Save the errors for each fold at each K value (10 folds x 9 K values = 90 values) as you loop through.\n",
    "\n",
    "Take a look at [sklearn's K-fold CV](http://scikit-learn.org/0.17/modules/generated/sklearn.cross_validation.KFold.html). Also, sklearn has it's own [K-NN implementation](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor). there is also an implementation of [mean squared error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html), though you'll have to take the root yourself. I've imported these for you already. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kValues = [1, 2, 3, 4, 5, 10, 20, 40, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folds = KFold(len(data_train_stand), n_folds = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18  19\n",
      "  20  21  22  23  24  26  27  28  29  30  31  32  33  34  35  37  38  39\n",
      "  40  41  42  43  44  45  46  47  48  49  50  51  53  54  55  57  58  59\n",
      "  60  61  63  64  65  66  67  69  70  71  72  73  74  75  76  77  78  79\n",
      "  80  82  83  84  85  86  87  88  89  90  91  92  93  94  97  98  99 100\n",
      " 101 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119\n",
      " 121 122 124 125 126 129 130 131 132 133 134 135 136 137 138 139 140 141\n",
      " 142 143 144 145 146 147 149 150 151 152 153 154 155 156 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 186 187 189 191 192 193 194 195 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207 208 209 210 212 213 214 215 216 217 218 219\n",
      " 221 222 224 225 226 227 228 230 231 232 233 234 235 236 237 239 240 241\n",
      " 242 243 244 245 246 247 249 250 251 252 253 254 257 258 259 260 261 262\n",
      " 263 264 265 266 267 268 271 272 273 274 275 276 277 278 280 281 282 283\n",
      " 284 285 286 287 288 289 290 291 292 293 295 296 297 298 300 301 302 303\n",
      " 304 305 306 307 308 309 310 311 312 313 315 316 317 318 319 320 321 322\n",
      " 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340\n",
      " 341 342 343 344 345 346 347 349 350 351 352 353]\n",
      "[  0   1   2   3   5   7  10  11  12  13  14  15  16  17  18  19  21  22\n",
      "  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  41\n",
      "  42  43  44  45  46  47  48  49  50  52  53  54  55  56  57  58  59  60\n",
      "  62  63  64  65  66  67  68  69  71  72  73  74  75  77  78  79  80  81\n",
      "  82  83  84  86  87  88  90  91  93  94  95  96  98  99 100 102 103 104\n",
      " 105 106 107 108 109 110 111 112 113 115 117 119 120 121 123 125 127 128\n",
      " 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146\n",
      " 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 163 164 165\n",
      " 166 167 168 169 170 171 172 174 175 176 177 178 180 181 182 183 184 185\n",
      " 186 187 188 189 190 191 192 193 194 195 196 198 199 202 203 204 205 206\n",
      " 207 208 209 210 211 212 214 215 216 217 218 219 220 221 222 223 224 225\n",
      " 227 228 229 230 231 232 233 235 236 237 238 239 240 241 242 243 244 245\n",
      " 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263\n",
      " 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281\n",
      " 282 283 284 285 287 288 289 290 291 292 293 294 295 296 297 298 299 301\n",
      " 302 303 304 305 308 309 310 311 313 314 315 316 317 318 319 320 321 322\n",
      " 323 324 325 326 327 328 329 330 331 332 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353]\n",
      "[  0   1   2   4   5   6   7   8   9  11  12  13  14  15  16  18  19  20\n",
      "  21  22  24  25  26  27  28  29  30  31  32  33  34  36  37  38  39  40\n",
      "  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58\n",
      "  59  60  61  62  63  64  65  66  67  68  69  70  72  74  75  76  78  79\n",
      "  80  81  82  83  84  85  87  88  89  90  91  92  93  94  95  96  97  98\n",
      "  99 101 102 103 105 107 108 109 110 112 113 114 115 116 117 118 119 120\n",
      " 121 122 123 124 125 126 127 128 130 132 133 134 136 137 138 139 140 141\n",
      " 142 143 144 146 147 148 150 152 153 155 156 157 159 160 161 162 163 164\n",
      " 165 166 167 171 173 174 175 177 179 180 181 182 183 184 185 186 187 188\n",
      " 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206\n",
      " 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224\n",
      " 225 226 227 228 229 230 231 232 233 234 235 236 237 238 240 241 242 243\n",
      " 244 246 247 248 250 251 252 253 254 255 256 257 258 259 260 261 262 263\n",
      " 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281\n",
      " 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299\n",
      " 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 318\n",
      " 320 321 322 323 325 327 328 329 330 331 332 333 335 336 337 338 339 340\n",
      " 341 343 344 345 346 347 348 349 350 351 352 353]\n",
      "[  0   1   2   3   4   6   7   8   9  10  11  12  13  14  15  17  18  19\n",
      "  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37\n",
      "  38  39  40  41  42  44  45  46  47  48  49  50  51  52  53  54  55  56\n",
      "  57  58  59  60  61  62  63  65  66  67  68  69  70  71  72  73  74  76\n",
      "  77  78  79  80  81  82  83  84  85  86  87  88  89  92  93  94  95  96\n",
      "  97  98 100 101 102 103 104 105 106 108 109 110 111 112 113 114 115 116\n",
      " 117 118 120 121 122 123 124 126 127 128 129 130 131 132 133 134 135 136\n",
      " 137 138 139 140 141 143 144 145 146 147 148 149 150 151 152 153 154 155\n",
      " 156 157 158 160 161 162 164 166 167 168 169 170 172 173 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 197 198\n",
      " 199 200 201 202 203 204 205 206 207 208 210 211 212 213 214 215 216 217\n",
      " 218 219 220 222 223 224 225 226 228 229 230 231 232 233 234 235 236 237\n",
      " 238 239 240 241 242 243 244 245 246 247 248 249 251 252 253 254 255 256\n",
      " 257 258 259 260 261 263 264 265 266 267 268 269 270 271 272 273 275 276\n",
      " 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294\n",
      " 295 296 297 298 299 300 301 303 304 305 306 307 308 309 310 312 313 314\n",
      " 316 317 318 319 320 321 322 323 324 325 326 327 329 330 331 333 334 336\n",
      " 339 340 341 342 343 344 345 347 348 349 351 352]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  20  21  22  23  24  25  26  27  28  29  30  31  34  35  36  37  38  40\n",
      "  41  42  43  44  45  46  47  48  49  51  52  53  54  55  56  61  62  63\n",
      "  64  65  67  68  69  70  71  72  73  75  76  77  78  79  80  81  82  83\n",
      "  84  85  86  87  89  90  91  92  93  94  95  96  97  98  99 100 101 102\n",
      " 103 104 106 107 108 109 110 111 112 113 114 116 118 119 120 121 122 123\n",
      " 124 125 126 127 128 129 130 131 132 133 134 135 136 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 192 193 196 197 198 199 200\n",
      " 201 202 203 204 205 206 207 208 209 210 211 213 214 215 216 217 218 219\n",
      " 220 221 222 223 224 225 226 227 229 230 231 232 233 234 235 236 238 239\n",
      " 240 242 243 244 245 246 248 249 250 251 252 253 254 255 256 257 258 259\n",
      " 260 262 263 264 265 266 267 268 269 270 273 274 275 276 277 278 279 280\n",
      " 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298\n",
      " 299 300 301 302 304 306 307 308 309 311 312 314 315 316 317 318 319 320\n",
      " 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338\n",
      " 340 341 342 343 344 345 346 347 348 349 350 351 353]\n",
      "[  0   1   2   3   4   5   6   8   9  10  12  15  16  17  18  19  20  21\n",
      "  22  23  24  25  26  27  28  29  31  32  33  34  35  36  38  39  40  41\n",
      "  42  43  44  45  47  49  50  51  52  53  56  57  58  59  60  61  62  63\n",
      "  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81\n",
      "  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  99 100 101\n",
      " 102 103 104 105 106 107 109 110 111 112 114 115 116 117 118 119 120 121\n",
      " 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139\n",
      " 140 141 142 143 145 147 148 149 151 152 153 154 155 156 157 158 159 160\n",
      " 161 162 163 164 165 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 187 188 189 190 191 192 194 195 196 197 199 200\n",
      " 201 202 203 204 205 206 207 209 210 211 212 213 214 215 216 217 218 220\n",
      " 221 222 223 224 225 226 227 228 229 230 231 233 234 235 236 237 238 239\n",
      " 240 241 242 243 244 245 247 248 249 250 251 252 254 255 256 258 259 260\n",
      " 261 262 263 264 267 268 269 270 271 272 273 274 275 276 277 278 279 280\n",
      " 281 282 283 284 285 286 287 288 289 290 292 294 296 297 298 299 300 301\n",
      " 302 303 304 305 306 307 308 310 311 312 313 314 315 316 317 318 319 320\n",
      " 321 322 323 324 326 327 328 329 330 331 332 333 334 335 336 337 338 339\n",
      " 340 341 342 343 344 345 346 347 348 349 350 352 353]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  28  30  32  33  35  36  37  38  39  40\n",
      "  41  42  43  45  46  47  48  49  50  51  52  54  55  56  57  58  59  60\n",
      "  61  62  64  65  66  67  68  69  70  71  73  74  75  76  77  78  79  81\n",
      "  82  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100\n",
      " 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118\n",
      " 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 137\n",
      " 138 139 140 142 143 144 145 146 147 148 149 150 151 152 153 154 155 157\n",
      " 158 159 160 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176\n",
      " 177 178 179 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 207 208 209 210 211 212 213 214 215 216\n",
      " 217 218 219 220 221 222 223 224 225 226 227 228 229 230 232 233 234 235\n",
      " 237 238 239 241 242 243 244 245 246 247 248 249 250 253 254 255 256 257\n",
      " 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275\n",
      " 278 279 280 281 284 285 286 287 288 289 290 291 292 293 294 295 297 298\n",
      " 299 300 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317\n",
      " 318 319 320 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337\n",
      " 338 339 340 341 342 344 346 348 349 350 351 352 353]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  22  23  25  26  27  28  29  30  31  32  33  34  35  36  37\n",
      "  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55\n",
      "  56  57  58  59  60  61  62  63  64  66  68  69  70  71  72  73  74  75\n",
      "  76  77  78  80  81  82  83  84  85  86  88  89  90  91  92  93  94  95\n",
      "  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113\n",
      " 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131\n",
      " 133 135 136 137 138 140 141 142 144 145 146 147 148 149 150 151 152 154\n",
      " 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173\n",
      " 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191\n",
      " 192 193 194 195 196 197 198 199 200 201 203 205 206 207 208 209 211 212\n",
      " 213 214 215 216 217 218 219 220 221 222 223 226 227 228 229 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 245 246 247 248 249 250 251 252\n",
      " 253 255 256 257 258 259 260 261 262 263 265 266 267 268 269 270 271 272\n",
      " 273 274 276 277 279 281 282 283 286 289 290 291 293 294 295 296 297 298\n",
      " 299 300 301 302 303 305 306 307 308 309 310 311 312 313 314 315 317 319\n",
      " 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337\n",
      " 338 339 342 343 344 345 346 347 348 350 351 352 353]\n",
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  16  17  18  19\n",
      "  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37\n",
      "  38  39  40  42  43  44  46  47  48  49  50  51  52  53  54  55  56  57\n",
      "  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75\n",
      "  76  77  78  79  80  81  82  83  85  86  87  88  89  90  91  92  93  95\n",
      "  96  97  98  99 100 101 102 103 104 105 106 107 108 111 112 113 114 115\n",
      " 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133\n",
      " 134 135 136 137 138 139 140 141 142 143 144 145 146 148 149 150 151 152\n",
      " 153 154 155 156 157 158 159 161 162 163 165 166 168 169 170 171 172 173\n",
      " 174 175 176 178 179 180 181 182 185 186 188 189 190 191 193 194 195 196\n",
      " 197 198 199 200 201 202 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 217 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 236\n",
      " 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254\n",
      " 255 256 257 261 262 264 265 266 269 270 271 272 273 274 275 276 277 278\n",
      " 279 280 282 283 284 285 286 287 288 289 291 292 293 294 295 296 299 300\n",
      " 301 302 303 304 305 306 307 309 310 311 312 313 314 315 316 317 318 319\n",
      " 321 322 324 325 326 327 328 329 330 331 332 333 334 335 337 338 339 340\n",
      " 341 342 343 344 345 346 347 348 349 350 351 352 353]\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  23  24  25  27  29  30  31  32  33  34  35  36  37  39\n",
      "  40  41  43  44  45  46  48  50  51  52  53  54  55  56  57  58  59  60\n",
      "  61  62  63  64  65  66  67  68  70  71  72  73  74  75  76  77  79  80\n",
      "  81  82  83  84  85  86  87  88  89  90  91  92  94  95  96  97  98  99\n",
      " 100 101 102 104 105 106 107 108 109 110 111 113 114 115 116 117 118 119\n",
      " 120 122 123 124 125 126 127 128 129 131 132 134 135 136 137 138 139 141\n",
      " 142 143 144 145 146 147 148 149 150 151 153 154 155 156 157 158 159 160\n",
      " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178\n",
      " 179 180 181 182 183 184 185 186 187 188 190 191 192 193 194 195 196 197\n",
      " 198 200 201 202 203 204 206 208 209 210 211 212 213 216 218 219 220 221\n",
      " 223 224 225 226 227 228 229 230 231 232 234 235 236 237 238 239 240 241\n",
      " 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261\n",
      " 262 263 264 265 266 267 268 269 270 271 272 274 275 276 277 278 279 280\n",
      " 281 282 283 284 285 286 287 288 290 291 292 293 294 295 296 297 298 299\n",
      " 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317\n",
      " 318 319 320 321 322 323 324 325 326 328 332 333 334 335 336 337 338 339\n",
      " 340 341 342 343 345 346 347 348 349 350 351 352 353]\n"
     ]
    }
   ],
   "source": [
    "for train_index, val_index in folds:\n",
    "    print(train_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sores = {}\n",
    "for k in kValues:\n",
    "    currentScores = []\n",
    "    for train_index, val_index in folds:\n",
    "        current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results\n",
    "\n",
    "Plot your training accuracy across all folds as a function of K. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
